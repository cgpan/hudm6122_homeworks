---
title: "HUDM6122 Homework_05"
author: "Chenguang Pan"
date: "2023-03-20"
output:
  pdf_document:
    toc: false
    toc_depth: 4
    number_sections: true
    keep_tex: true
    highlight: tango
---

```{r setup, include=FALSE}
## Global options
knitr::opts_chunk$set(
  cache = TRUE,
  prompt = TRUE,
  comment = '',
  collapse = TRUE,
  warning = FALSE,
  message = FALSE)
```   

## Github Address  
All my latest homework can be found on Github: https://github.com/cgpan/hudm6122_homeworks . Thanks for checking if interested.


## Ex 5.1  
*Show how the result rises from the assumptions of uncorrelated factors, independence of the specific variates, and independence of common factors and specific variances. What form does take if the factors are allowed to be correlated?*  
  
  
**MY SOLUTION:**   
Based on the assumption of Exploratory Factor Analysis(EFA), a set of observed variables **x** assumed to be linked to a set of latent variables **f**. Therefore, we can have a regression model in matrix form $$\boldsymbol x=\boldsymbol\Lambda \boldsymbol f + \boldsymbol u$$, where $\boldsymbol \Lambda$ is a $q \times k$ matrix of factor loadings (a.k.a., the coefficients of the regression model), and the *u* is the vector of unexplained error of each observed variables.  

Let's take the variance of the formula above $$V(\boldsymbol x) = V(\boldsymbol\Lambda \boldsymbol f + \boldsymbol u)$$. Based on the operation rule of variance, like $$V(a+b)= V(a) + V(b) + 2Cov(ab)$$, we combined the two formulas above, then $$V(\boldsymbol x) = V(\boldsymbol\Lambda \boldsymbol f + \boldsymbol u) = V(\boldsymbol\Lambda \boldsymbol f) + V(\boldsymbol u) + 2 Cov(\boldsymbol\Lambda \boldsymbol f \boldsymbol u)$$. Since the we assumed that the error terms are uncorrelated with the factors, therefore the $Cov(\boldsymbol\Lambda \boldsymbol f \boldsymbol u)=0$. Then, we can continue to drive the variance formula as $$V(\boldsymbol x) = V(\boldsymbol\Lambda \boldsymbol f) + V(\boldsymbol u) = \boldsymbol\Lambda V(\boldsymbol f) \boldsymbol\Lambda^T + \Psi$$. In addition, we assumed that the factors are uncorrelated with each other. The $V(\boldsymbol f)$ is actually an identity matrix. Therefore, the formula can be written as $$V(\boldsymbol x) = \boldsymbol\Lambda V(\boldsymbol f) \boldsymbol\Lambda^T + \Psi = \boldsymbol\Lambda \boldsymbol\Lambda^T + \Psi$$. Finally, the formula can be written as $$\boldsymbol \Sigma = \boldsymbol\Lambda \boldsymbol\Lambda^T + \Psi$$.  

If we allow the factors to be correlated with each other, then the $V(\boldsymbol f)$ is not an identity matrix. Let's use the greek letter $\Phi$ to represent the variance matrix of loadings **f**. Thus, the formula should be $$\boldsymbol \Sigma = \boldsymbol\Lambda \boldsymbol \Phi \boldsymbol\Lambda^T + \Psi$$.  

## Ex 5.2  
*Show that the communalities in a factor analysis model are unaffected by the transformation ...*  
  
**MY SOLUTION:**   
This question mentioned that we need to use the transformed factor loadings $\boldsymbol \Lambda ^* = \boldsymbol \Lambda \boldsymbol M$. Let's assume that $\boldsymbol M$ is an $k \times k$ orthogonal matrix. We can re-write the the basic regression equation linking the observed and the factors as: $$\boldsymbol x=(\boldsymbol\Lambda \boldsymbol M)( \boldsymbol M^T \boldsymbol f) + \boldsymbol u$$.   
Using the rule of variance, we can have $$\boldsymbol \Sigma = (\boldsymbol\Lambda \boldsymbol M)(\boldsymbol\Lambda \boldsymbol M)^T + \Psi$$. Since the $\boldsymbol M$ is a orthogonal matrix and $\boldsymbol M \boldsymbol M^T = \boldsymbol I$. Therefore, the variance equation can be written as  $$\boldsymbol \Sigma = \boldsymbol\Lambda \boldsymbol\Lambda^T + \Psi$$. That is, the transformed factor loadings $\boldsymbol \Lambda ^* = \boldsymbol \Lambda \boldsymbol M$ will not influence the communalities (i.e., $\boldsymbol\Lambda \boldsymbol\Lambda^T$) in the a factor analysis model.  


## Ex 5.3    
*Give a formula for the proportion of variance explained by the jth factor estimated by the principal factor approach.*  
  
**MY SOLUTION:**   
The proportion of variance explained by the jth factor represents the proportion of the total variance in the observed variables that is accounted for by that factor alone. Therefore, the formula could be $$Proportion_j = \frac {\sum_{i=1}^{q} \lambda_{ij}^2}{\boldsymbol \Lambda \boldsymbol \Lambda ^T}$$.  

## Ex 5.4    
*Apply the factor analysis model separately to the life expectancies of men and women and compare the results.*  
  
**MY SOLUTION:**  

For this question, I present two methods to run factor analysis. The first one is similar to the method introduced in textbook. The second one is a more rigorous method including the initial dataset checking, scree plot analysis, and parallel analysis.  

### Method 1: Using the similar method introduced by textbook
The textbook does not provide the original dataset. Based on the code in the `MVA`, I create the dataset via a separated r file named "HW05 Test". This file created the `life.rdata` and `life.csv` dataset in the same file folder. 
```{r}
load("life.rdata")
head(life)

# subset the male and female dataset
life_male <- life[,1:4]
life_female <- life[,5:8]

# test the number of factors needed for the male and female dataset separately
sapply(1, function(f)
  factanal(life_male, factors=f, method="mle")$PVAL)
sapply(1, function(f)
  factanal(life_female, factors=f, method="mle")$PVAL)
```

When test the number of the factors from 1 to larger number, there is always a warning that `N factors are too many for N variables`. More details can be found on Page 143 of the textbook or here https://stats.stackexchange.com/questions/593452/efa-n-factors-are-too-many-for-n-variables

The results suggest that an one-factor solution might be adequate to account for the observed covariances in the data.   

Next, I run the one-factor solution for both male and female datasets.
```{r}
factanal(life_male, factors = 1, method="mle")
factanal(life_female, factors = 1, method="mle")
```  
The result shows that for the one-factor solution in the male dataset, it captures the most variance of age 50 or older. But comparing to the EFA on complete dataset, this factor does not have a very clear indication for what properties it covers. The factor analysis on female dataset has the similar situation.

Is one-factor really appropriate here? Since the P-value is still significant. I plan to try another method to search for the right number of factors.

### Method 2: Solve this question in another way  
Before running EFA, I run several tests to ensure that this dataset is good for factor analysis.
```{r}
# install.packages("psych")
library(psych)
# get the correlation matrix
life_male_cor <- cor(life_male)
life_female_cor <- cor(life_female)

# The Kaiser-Meyer-Olkin (KMO) used to measure sampling adequacy 
# is a better measure of factorability.
KMO(life_male_cor)
KMO(life_female_cor)
```  
According to Kaiser's (1974) guidelines, a suggested cutoff for determining the factorability of the sample data is KMO >= 60. The total KMOs are 0.66 and 0.63, indicating that, based on this test, we can probably conduct a factor analysis.  

Next, Bartlett's Test of Sphericity compares an observed correlation matrix to the identity matrix. Essentially it checks to see if there is a certain redundancy between the variables that we can summarize with a few number of factors. The null hypothesis of the test is that the variables are orthogonal, i.e. not correlated.  
```{r}
# run Bartlett's Test of Sphericity
cortest.bartlett(life_male_cor)$p.value
cortest.bartlett(life_female_cor)$p.value
```  
Small p values (< 0.05) of the significance level indicate that a factor analysis may be useful with our data.  
```{r}
# get the determinants for both correlation matrix
det(life_male_cor)
det(life_female_cor)
```
Finally, we have positive determinants, which means the factor analysis will probably run.  

Here, I begin to run EFA by using `fa()` function and make a scree plot to determine the number of factors.
```{r}
library(ggplot2)
# run factor analysis using fa() function
male_fa <- fa(life_male, 
              nfactors = ncol(life_male_cor), 
              rotate = "varimax")
efa_model <- fa(life_male, nfactors = 2, rotate = "varimax")
# to get the number of factors
n_factors <- length(male_fa$e.values)

# to store the data
scree <- data.frame(Factor_n = as.factor(1:n_factors), 
                    Eigenvalue = male_fa$e.values)
# draw scree plot using ggplot2
ggplot(scree, aes(x = Factor_n, y = Eigenvalue, group = 1)) + 
      geom_point() + geom_line() +
      xlab("Number of factors") +
      ylab("Initial eigenvalue") +
      labs( title = "Scree Plot", 
            subtitle = "(Based on the unreduced correlation matrix)")
```  
From the scree plot, 1 factors maybe appropriate for the male dataset. Since only one factor's eigenvalue is greater than 1. However, the second factor is above .7. It may be appropriate too. 

*Tips* Why I set eigenvalue = 1 as a cutoff?  
Here, the eigenvalue is a measure of the amount of variance in the observed variables that is accounted for by each factor. If the eigenvalue of a factor is less than 1, it indicates that the factor explains less variance than one of the original variables and, therefore, does not contribute significantly to the explanation of the common variance among the variables.


Parallel analysis is a method for determining the number of components or factors to retain from pca or factor analysis.I also use perform parallel analysis to determine the factors. notice the results in the console will provide the suggestion.

From the returned result under the scree plot, it suggests that 2-factor solution may be good for the male dataset.
```{r}
parallel <- fa.parallel(life_male_cor)
```  

Using the same method on the female dataset.
```{r,fig.show='hold',out.width="50%",out.height= "50%"}
# run factor analysis using fa() function
female_fa <- fa(life_female, 
              nfactors = ncol(life_female_cor), 
              rotate = "varimax")
# to get the number of factors
n_factors <- length(female_fa$e.values)

# to store the data
scree <- data.frame(Factor_n = as.factor(1:n_factors), 
                    Eigenvalue = female_fa$e.values)
par(mfrow=c(1, 2))
# draw scree plot using ggplot2
ggplot(scree, aes(x = Factor_n, y = Eigenvalue, group = 1)) + 
      geom_point() + geom_line() +
      xlab("Number of factors") +
      ylab("Initial eigenvalue") +
      labs( title = "Scree Plot", 
            subtitle = "(Based on the unreduced correlation matrix)")

parallel <- fa.parallel(life_male_cor)
```  
The results also suggest that 2-factor solution is good for female dataset.

Finally, I checked the factor loadings of both male and female datasets.
```{r}
male_fa$loadings
female_fa$loadings
```  
This two-factor solution looks more reasonable than the one-factor. Since in both dataset, the first factor captures the 0-25 age's life expectations and the second factor covers the 50-75 age's life expectations. We can call the first factor "Life force under middle age" and the second factor "life force of or above middle age".


## Ex 5.6    
*The matrix below shows the correlations between ratings on nine statements about pain made by 123 people suffering from extreme pain. Each statement was scored on a scale from 1 to 6, ranging from agreement to disagreement. The nine pain statements were as follows:*  
  
**MY SOLUTION:**  
First, to change the lower triangular matrix into the complete correlation matrix.
```{r}
library(Matrix)
# import the correlation matrix
corr_lower <- matrix(c(1, 0, 0, 0, 0, 0, 0, 0, 0,
                      -0.04, 1, 0,0,0,0,0,0,0,
                      0.61, -0.07, 1,0,0,0,0,0,0,
                      0.45, -0.12, 0.59, 1,0,0,0,0,0,
                      0.03, 0.49, 0.03, -0.08, 1,0,0,0,0,
                      -0.29, 0.43, -0.13, -0.21, 0.47, 1,0,0,0,
                      -0.30, 0.30, -0.24, -0.19, 0.41, 0.63,1,0,0, 
                      0.45, -0.31,0.59,0.63,-0.14,-0.13,-0.26,1,0,
                      0.30,-0.17,-0.32,0.37,-0.24,-0.15,-0.29,0.40,1),9,9, byrow = T)
# generate a complete correlation matrix
corr_symmetric <- forceSymmetric(corr_lower, uplo="L")
```   
The correlation matrix looks good. Next, I run the PCA first.  
```{r}
# run the PCA first
# use prcomp to calculate the principal components
pca <- prcomp(corr_symmetric, scale. = FALSE)
# get the PCA results
summary(pca)
# draw the scree plot 
plot(pca, type = "l", 
     main = "Scree Plot")
```  
The scree-plot shows that 3 principle components may be appropriate. However, based on the results from the PCA analysis, the first two components can explain 88.63% variance of the total. Therefore, I choose the first two to represent the data.  

Next, I run maximum likelihood factor analysis.
```{r, eval=FALSE}
# explore the number of factors
as.matrix(corr_symmetric)
sapply(1:6, function(f) factanal(covmat=as.matrix(corr_symmetric), factors=f, method="mle", n.obs = 123)$PVAL)
```


