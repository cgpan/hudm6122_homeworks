---
title: "HUDM6122 Homework_05"
author: "Chenguang Pan"
date: "2023-03-20"
output:
  pdf_document:
    toc: false
    toc_depth: 4
    number_sections: true
    keep_tex: true
    highlight: tango
---

```{r setup, include=FALSE}
## Global options
knitr::opts_chunk$set(
  cache = TRUE,
  prompt = TRUE,
  comment = '',
  collapse = TRUE,
  warning = FALSE,
  message = FALSE)
```   

## Github Address  
All my latest homework can be found on Github: https://github.com/cgpan/hudm6122_homeworks . Thanks for checking if interested.


## Ex 5.1  
*Show how the result rises from the assumptions of uncorrelated factors, independence of the specific variates, and independence of common factors and specific variances. What form does take if the factors are allowed to be correlated?*  
  
  
**MY SOLUTION:**   
Based on the assumption of Exploratory Factor Analysis(EFA), a set of observed variables **x** assumed to be linked to a set of latent variables **f**. Therefore, we can have a regression model in matrix form $$\boldsymbol x=\boldsymbol\Lambda \boldsymbol f + \boldsymbol u$$, where $\boldsymbol \Lambda$ is a $q \times k$ matrix of factor loadings (a.k.a., the coefficients of the regression model), and the *u* is the vector of unexplained error of each observed variables.  

Let's take the variance of the formula above $$V(\boldsymbol x) = V(\boldsymbol\Lambda \boldsymbol f + \boldsymbol u)$$. Based on the operation rule of variance, like $$V(a+b)= V(a) + V(b) + 2Cov(ab)$$, we combined the two formulas above, then $$V(\boldsymbol x) = V(\boldsymbol\Lambda \boldsymbol f + \boldsymbol u) = V(\boldsymbol\Lambda \boldsymbol f) + V(\boldsymbol u) + 2 Cov(\boldsymbol\Lambda \boldsymbol f \boldsymbol u)$$. Since the we assumed that the error terms are uncorrelated with the factors, therefore the $Cov(\boldsymbol\Lambda \boldsymbol f \boldsymbol u)=0$. Then, we can continue to drive the variance formula as $$V(\boldsymbol x) = V(\boldsymbol\Lambda \boldsymbol f) + V(\boldsymbol u) = \boldsymbol\Lambda V(\boldsymbol f) \boldsymbol\Lambda^T + \Psi$$. In addition, we assumed that the factors are uncorrelated with each other. The $V(\boldsymbol f)$ is actually an identity matrix. Therefore, the formula can be written as $$V(\boldsymbol x) = \boldsymbol\Lambda V(\boldsymbol f) \boldsymbol\Lambda^T + \Psi = \boldsymbol\Lambda \boldsymbol\Lambda^T + \Psi$$. Finally, the formula can be written as $$\boldsymbol \Sigma = \boldsymbol\Lambda \boldsymbol\Lambda^T + \Psi$$.  

If we allow the factors to be correlated with each other, then the $V(\boldsymbol f)$ is not an identity matrix. Let's use the greek letter $\Phi$ to represent the variance matrix of loadings **f**. Thus, the formula should be $$\boldsymbol \Sigma = \boldsymbol\Lambda \boldsymbol \Phi \boldsymbol\Lambda^T + \Psi$$.  

## Ex 5.2  
*Show that the communalities in a factor analysis model are unaffected by the transformation ...*  
  
**MY SOLUTION:**   
This question mentioned that we need to use the transformed factor loadings $\boldsymbol \Lambda ^* = \boldsymbol \Lambda \boldsymbol M$. Let's assume that $\boldsymbol M$ is an $k \times k$ orthogonal matrix. We can re-write the the basic regression equation linking the observed and the factors as: $$\boldsymbol x=(\boldsymbol\Lambda \boldsymbol M)( \boldsymbol M^T \boldsymbol f) + \boldsymbol u$$.   
Using the rule of variance, we can have $$\boldsymbol \Sigma = (\boldsymbol\Lambda \boldsymbol M)(\boldsymbol\Lambda \boldsymbol M)^T + \Psi$$. Since the $\boldsymbol M$ is a orthogonal matrix and $\boldsymbol M \boldsymbol M^T = \boldsymbol I$. Therefore, the variance equation can be written as  $$\boldsymbol \Sigma = \boldsymbol\Lambda \boldsymbol\Lambda^T + \Psi$$. That is, the transformed factor loadings $\boldsymbol \Lambda ^* = \boldsymbol \Lambda \boldsymbol M$ will not influence the communalities (i.e., $\boldsymbol\Lambda \boldsymbol\Lambda^T$) in the a factor analysis model.  


## Ex 5.3    
*Give a formula for the proportion of variance explained by the jth factor estimated by the principal factor approach.*  
  
**MY SOLUTION:**   
The proportion of variance explained by the jth factor represents the proportion of the total variance in the observed variables that is accounted for by that factor alone. Therefore, the formula could be $$Proportion_j = \frac {\sum_{i=1}^{q} \lambda_{ij}^2}{\boldsymbol \Lambda \boldsymbol \Lambda ^T}$$.  

## Ex 5.4    
*Apply the factor analysis model separately to the life expectancies of men and women and compare the results.*  
  
**MY SOLUTION:**   
The textbook does not provide the original dataset. Based on the code in the `MVA`, I create the dataset via a separated r file named "HW05 Test". This file created the `life.rdata` and `life.csv` dataset in the same file folder. 
```{r}
load("life.rdata")
head(life)

# subset the male and female dataset
life_male <- life[,1:4]
life_female <- life[,5:8]

# test the number of factors needed for the male and female dataset separately
sapply(1, function(f)
  factanal(life_male, factors=f, method="mle")$PVAL)
sapply(1, function(f)
  factanal(life_female, factors=f, method="mle")$PVAL)
```

When test the number of the factors from 1 to larger number, there is always a warning that `N factors are too many for N variables`. More details can be found on Page 143 of the textbook or here https://stats.stackexchange.com/questions/593452/efa-n-factors-are-too-many-for-n-variables

The results suggest that an one-factor solution might be adequate to account for the observed covariances in the data.   

Next, I run the one-factor solution for both male and female datasets.
```{r}
factanal(life_male, factors = 1, method="mle")
factanal(life_female, factors = 1, method="mle")
```


## Ex 5.6    
*The matrix below shows the correlations between ratings on nine statements about pain made by 123 people suffering from extreme pain. Each statement was scored on a scale from 1 to 6, ranging from agreement to disagreement. The nine pain statements were as follows:*  
  
**MY SOLUTION:**  
First, to change the lower triangular matrix into the complete correlation matrix.
```{r}
library(Matrix)
# import the correlation matrix
corr_lower <- matrix(c(1, 0, 0, 0, 0, 0, 0, 0, 0,
                      -0.04, 1, 0,0,0,0,0,0,0,
                      0.61, -0.07, 1,0,0,0,0,0,0,
                      0.45, -0.12, 0.59, 1,0,0,0,0,0,
                      0.03, 0.49, 0.03, -0.08, 1,0,0,0,0,
                      -0.29, 0.43, -0.13, -0.21, 0.47, 1,0,0,0,
                      -0.30, 0.30, -0.24, -0.19, 0.41, 0.63,1,0,0, 
                      0.45, -0.31,0.59,0.63,-0.14,-0.13,-0.26,1,0,
                      0.30,-0.17,-0.32,0.37,-0.24,-0.15,-0.29,0.40,1),9,9, byrow = T)
# generate a complete correlation matrix
corr_symmetric <- forceSymmetric(corr_lower, uplo="L")
```   
The correlation matrix looks good. Next, I run the PCA first.  
```{r}
# run the PCA first
# use prcomp to calculate the principal components
pca <- prcomp(corr_symmetric, scale. = FALSE)
# get the PCA results
summary(pca)
# draw the scree plot 
plot(pca, type = "l", 
     main = "Scree Plot")
```  
The scree-plot shows that 3 principle components may be appropriate. However, based on the results from the PCA analysis, the first two components can explain 88.63% variance of the total. Therefore, I choose the first two to represent the data.  

Next, I run maximum likelihood factor analysis.
```{r}
# explore the number of factors
as.matrix(corr_symmetric)
sapply(1:6, function(f) factanal(covmat=as.matrix(corr_symmetric), factors=f, method="mle", n.obs = 123)$PVAL)
```


